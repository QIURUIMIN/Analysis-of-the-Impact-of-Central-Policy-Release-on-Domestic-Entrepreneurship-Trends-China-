{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def getHTML(u):\n",
    "    try:\n",
    "        r = requests.get(u,timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \n",
    "\n",
    "def getContent(u):\n",
    "    html = getHTML(u)\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    title = soup.select('div.mbtitle')\n",
    "    paras_tmp = soup.select('p')\n",
    "    paras = paras_tmp[3:]\n",
    "    return paras\n",
    " \n",
    "def saveFile(text):\n",
    "    f=open('try.txt','w',encoding='utf-8')\n",
    "    for t in text:\n",
    "        if len(t) > 0:\n",
    "            f.writelines(t.get_text() + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "def finish(u):\n",
    "    text = getContent(u)\n",
    "    saveFile(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='gbk'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'pages_content'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        for x in ['wrap','zw-con']:\n",
    "            news_list = soup.find('div', {'class': x})\n",
    "            if news_list!=None:\n",
    "                break\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    lst = []\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba\n",
    "import requests\n",
    "df = pd.read_excel(r'C:\\Users\\HONEYMOON\\Desktop\\规划\\中央创业政策2.xlsx')\n",
    "web = df['网址']\n",
    "web_lst = []\n",
    "for w in web:\n",
    "        web_lst.append(w)\n",
    "    lst = []\n",
    "    dics_counts={}\n",
    "    for u in web_lst[139:141]:\n",
    "        print(web_lst.index(u)+2)\n",
    "        try:\n",
    "                web(u)\n",
    "            except:\n",
    "                try:\n",
    "                    finish(u)\n",
    "                except:\n",
    "                    print(u)\n",
    "            txt = open('try.txt',encoding = 'utf-8').read()\n",
    "            #加载停用词表\n",
    "            stopwords = [line.strip() for line in open(r\"D:\\try\\停用词.txt\",encoding=\"utf-8\").readlines()]\n",
    "            words  = jieba.lcut(txt)\n",
    "            counts = {}\n",
    "            lst_top3=[]\n",
    "            for word in words:\n",
    "            #不在停用词表中\n",
    "                if word not in stopwords:\n",
    "                #不统计字数为一的词\n",
    "                    if len(word) == 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        counts[word] = counts.get(word,0) + 1\n",
    "            items = list(counts.items())\n",
    "            items.sort(key=lambda x:x[1], reverse=True)\n",
    "            for i in range(3):\n",
    "                word, count = items[i]\n",
    "                if word.isdigit():\n",
    "                    continue\n",
    "                else:\n",
    "                    dics_counts[word]=dics_counts.get(word,0)+1\n",
    "                    lst_top3.append(word)\n",
    "            lst.append(lst_top3)\n",
    "\n",
    "            print(dics_counts)\n",
    "            print(lst_top3)\n",
    "            print(lst)\n",
    "            '''print(counts)\n",
    "            print(items)'''\n",
    "            print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import requests\n",
    "dics={}\n",
    "lst=[]\n",
    "\n",
    "\n",
    "def txt_to_sta():\n",
    "    dics={}\n",
    "    lst=[]\n",
    "    lst0=[]\n",
    "    # fil1=input()\n",
    "    txt = open('网站.txt', encoding=\"utf-8\").read()\n",
    "    #加载停用词表\n",
    "    stopwords = [line.strip() for line in open(\"2.txt\",encoding=\"utf-8\").readlines()]\n",
    "    words  = jieba.lcut(txt)\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "    #不在停用词表中\n",
    "        if word not in stopwords:\n",
    "        #不统计字数为一的词\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                counts[word] = counts.get(word,0) + 1\n",
    "    items = list(counts.items())\n",
    "    items.sort(key=lambda x:x[1], reverse=True)\n",
    "    for i in range(3):\n",
    "        word, count = items[i]\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            dics[word]=dics.get(word,0)+1\n",
    "            lst0.append(word)\n",
    "    lst.append(lst0)\n",
    "    print(dics)\n",
    "    gramlst=[]\n",
    "    savelst=[]\n",
    "    dics=sorted(dics.items(),key=lambda d:d[1])\n",
    "    for x in dics:\n",
    "        if x[1]<10:\n",
    "            continue\n",
    "        else:\n",
    "            savelst.append(x[0])\n",
    "    for x in dics:\n",
    "        dic0={}\n",
    "        if x[0] in savelst:\n",
    "            for z in dics:\n",
    "                if z[0] in savelst:# and z[0]!=x[0]\n",
    "                    dic0[z[0]]=0\n",
    "            gramlst.append(x[0])\n",
    "            for t in lst:\n",
    "                if x[0] in t:\n",
    "                    for y in t:\n",
    "                        if y!=x[0] and y in savelst:\n",
    "                            dic0[y]=dic0.get(y,0)+1\n",
    "                else:\n",
    "                    continue\n",
    "            gramlst.append(dic0)\n",
    "    for i in range(len(gramlst)):\n",
    "        if i%2==0:\n",
    "            print(gramlst[i])\n",
    "        else:\n",
    "            continue\n",
    "    for i in range(len(gramlst)):\n",
    "        if i%2==0:\n",
    "            print(gramlst[i])\n",
    "        else:\n",
    "            for x in gramlst[i].values():\n",
    "                print(x)\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='utf-8'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'pages_content'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        for x in ['wrap','zw-con']:\n",
    "            news_list = soup.find('div', {'class': x})\n",
    "            if news_list!=None:\n",
    "                break\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)\n",
    "\n",
    "\n",
    "def web2(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='GB2312'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'zw-con'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        main_text_element = soup.find('td', {'class': 'black14'})\n",
    "        for style_element in soup.find_all('style'):\n",
    "            style_element.decompose()\n",
    "        \n",
    "        news_list=main_text_element\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)\n",
    "\n",
    "def main():\n",
    "    from time import time\n",
    "    t0=time()\n",
    "    txt0 = open('网站.txt', encoding=\"utf-8\")\n",
    "    while True:\n",
    "        line0=txt0.readline()\n",
    "        if not line0:\n",
    "            break\n",
    "        else:\n",
    "            line=line0.replace('\\n','')\n",
    "            u=line\n",
    "            if line!=''and 'ndrc' not in line and 'mohrss' not in line:\n",
    "                try:\n",
    "                    web(u)\n",
    "                    txt_to_sta()\n",
    "                except:\n",
    "                    try:\n",
    "                        web2(u)\n",
    "                        txt_to_sta()\n",
    "                    except:\n",
    "                        print(u)\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.gov.cn/zhengce/zhengceku/2015-05/01/content_9688.htm\n",
      "http://www.gov.cn/zhengce/2014-10/16/content_5023512.htm\n",
      "http://www.gov.cn/zhengce/2014-10/15/content_5023780.htm\n",
      "http://www.gqt.org.cn/documents/zqf/201410/t20141016_715998.htm\n",
      "http://www.gov.cn/zhengce/2014-10/11/content_5023511.htm\n",
      "http://www.gov.cn/zhengce/2014-10/10/content_5023815.htm\n",
      "http://www.gqt.org.cn/documents/zqbf/201409/t20140930_714378.htm\n",
      "http://www.gqt.org.cn/documents/zqf/201409/t20140919_712602.htm\n",
      "http://www.gov.cn/zhengce/2014-09/05/content_5023441.htm\n",
      "http://www.gqt.org.cn/documents/zqf/201409/t20140904_708954.htm\n",
      "http://www.gqt.org.cn/documents/zqlf/201408/t20140826_707597.htm\n",
      "http://www.gov.cn/zhengce/2014-08/18/content_5026158.htm\n",
      "http://www.gov.cn/zhengce/2014-07/23/content_5023757.htm\n",
      "http://www.gqt.org.cn/documents/zqlf/201407/t20140721_702205.htm\n",
      "http://www.gqt.org.cn/documents/zqlf/201407/t20140707_700103.htm\n",
      "http://www.gov.cn/zhengce/2014-06/12/content_5023808.htm\n",
      "http://www.gov.cn/zhengce/2014-05/30/content_5023436.htm\n",
      "http://www.gov.cn/zhengce/2014-05/22/content_5023770.htm\n",
      "http://www.csrc.gov.cn/pub/zjhpublic/G00306201/201405/t20140529_255115.htm\n",
      "http://www.gov.cn/zhengce/zhengceku/2014-05/13/content_8802.htm\n",
      "http://www.gov.cn/zhengce/2014-05/13/content_5023552.htm\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import requests\n",
    "dics={}\n",
    "lst=[]\n",
    "def txt_to_sta():\n",
    "    lst0=[]\n",
    "    # fil1=input()\n",
    "    txt = open('try.txt', encoding = \"unicode_escape\").read()\n",
    "    #加载停用词表\n",
    "    stopwords = [line.strip() for line in open(\"2.txt\",encoding=\"utf-8\").readlines()]\n",
    "    words  = jieba.lcut(txt)\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "    #不在停用词表中\n",
    "        if word not in stopwords:\n",
    "        #不统计字数为一的词\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                counts[word] = counts.get(word,0) + 1\n",
    "    items = list(counts.items())\n",
    "    items.sort(key=lambda x:x[1], reverse=True)\n",
    "    for i in range(3):\n",
    "        word, count = items[i]\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            dics[word]=dics.get(word,0)+1\n",
    "            lst0.append(word)\n",
    "    lst.append(lst0)\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='utf-8'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'pages_content'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        for x in ['wrap','zw-con']:\n",
    "            news_list = soup.find('div', {'class': x})\n",
    "            if news_list!=None:\n",
    "                break\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)\n",
    "\n",
    "\n",
    "def web2(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='GB2312'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'zw-con'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        main_text_element = soup.find('td', {'class': 'black14'})\n",
    "        for style_element in soup.find_all('style'):\n",
    "            style_element.decompose()\n",
    "        \n",
    "        news_list=main_text_element\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.gov.cn/zhengce/zhengceku/2015-05/01/content_9688.htm\n",
      "http://www.gov.cn/zhengce/2014-10/16/content_5023512.htm\n",
      "http://www.gov.cn/zhengce/2014-10/15/content_5023780.htm\n",
      "http://www.gov.cn/zhengce/2014-10/11/content_5023511.htm\n",
      "http://www.gov.cn/zhengce/2014-10/10/content_5023815.htm\n",
      "http://www.gqt.org.cn/documents/zqbf/201409/t20140930_714378.htm\n",
      "http://www.gqt.org.cn/documents/zqf/201409/t20140919_712602.htm\n",
      "http://www.gov.cn/zhengce/2014-09/05/content_5023441.htm\n",
      "http://www.gqt.org.cn/documents/zqf/201409/t20140904_708954.htm\n",
      "http://www.gqt.org.cn/documents/zqlf/201408/t20140826_707597.htm\n",
      "http://www.gov.cn/zhengce/2014-08/18/content_5026158.htm\n",
      "http://www.gov.cn/zhengce/2014-07/23/content_5023757.htm\n",
      "http://www.gqt.org.cn/documents/zqlf/201407/t20140721_702205.htm\n",
      "http://www.gqt.org.cn/documents/zqlf/201407/t20140707_700103.htm\n",
      "http://www.gov.cn/zhengce/2014-06/12/content_5023808.htm\n",
      "http://www.gov.cn/zhengce/2014-05/30/content_5023436.htm\n",
      "http://www.gov.cn/zhengce/2014-05/22/content_5023770.htm\n",
      "http://www.csrc.gov.cn/pub/zjhpublic/G00306201/201405/t20140529_255115.htm\n",
      "http://www.gov.cn/zhengce/zhengceku/2014-05/13/content_8802.htm\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0=time()\n",
    "txt0 = open('网站.txt', encoding=\"utf-8\")\n",
    "while True:\n",
    "    line0=txt0.readline()\n",
    "    if not line0:\n",
    "        break\n",
    "    else:\n",
    "        line=line0.replace('\\n','')\n",
    "        u=line\n",
    "        if line!=''and 'ndrc' not in line and 'mohrss' not in line:\n",
    "            try:\n",
    "                web(u)\n",
    "                txt_to_sta()\n",
    "            except:\n",
    "                try:\n",
    "                    web2(u)\n",
    "                    txt_to_sta()\n",
    "                except:\n",
    "                    print(u)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramlst=[]\n",
    "savelst=[]\n",
    "dics=sorted(dics.items(),key=lambda d:d[1])\n",
    "for x in dics:\n",
    "    if x[1]<10:\n",
    "        continue\n",
    "    else:\n",
    "        savelst.append(x[0])\n",
    "for x in dics:\n",
    "    dic0={}\n",
    "    if x[0] in savelst:\n",
    "        for z in dics:\n",
    "            if z[0] in savelst:# and z[0]!=x[0]\n",
    "                dic0[z[0]]=0\n",
    "        gramlst.append(x[0])\n",
    "        for t in lst:\n",
    "            if x[0] in t:\n",
    "                for y in t:\n",
    "                    if y!=x[0] and y in savelst:\n",
    "                        dic0[y]=dic0.get(y,0)+1\n",
    "            else:\n",
    "                continue\n",
    "        gramlst.append(dic0)\n",
    "for i in range(len(gramlst)):\n",
    "    if i%2==0:\n",
    "        print(gramlst[i])\n",
    "    else:\n",
    "        continue\n",
    "for i in range(len(gramlst)):\n",
    "    if i%2==0:\n",
    "        print(gramlst[i])\n",
    "    else:\n",
    "        for x in gramlst[i].values():\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [大象, 狮子, 老虎, 猪]\n",
      "1     [狗, 狮子, 孔雀, 猫咪]\n",
      "2    [大象, 北极熊, 老虎, 猪]\n",
      "3     [大象, 狗, 老虎, 小鸡]\n",
      "4     [孔雀, 狮子, 老虎, 猪]\n",
      "dtype: object\n",
      "     北极熊  大象  孔雀  小鸡  狗  狮子  猪  猫咪  老虎\n",
      "北极熊    0   0   0   0  0   0  0   0   0\n",
      "大象     0   0   0   0  0   0  0   0   0\n",
      "孔雀     0   0   1   0  0   1  1   0   1\n",
      "小鸡     0   0   0   0  0   0  0   0   0\n",
      "狗      0   0   0   0  0   0  0   0   0\n",
      "狮子     0   0   1   0  0   1  1   0   1\n",
      "猪      0   0   1   0  0   1  1   0   1\n",
      "猫咪     0   0   0   0  0   0  0   0   0\n",
      "老虎     0   0   1   0  0   1  1   0   1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def gx_matrix(vol_li):\n",
    "    # 整合一下，输入是df列，输出直接是矩阵\n",
    "    names = locals()\n",
    "    all_col0 = []   # 用来后续求所有字段的集合\n",
    "    for row in vol_li:\n",
    "        all_col0 += row\n",
    "    for each in row:  # 对每行的元素进行处理，存在该字段字典的话，再进行后续判断，否则创造该字段字典\n",
    "        try:\n",
    "            for each1 in row:  # 对已存在字典，循环该行每个元素，存在则在已有次数上加一，第一次出现创建键值对“字段：1”\n",
    "                try:\n",
    "                    names['dic_' + each][each1] = names['dic_' + each][each1] + 1  # 尝试，一起出现过的话，直接加1\n",
    "                except:\n",
    "                    names['dic_' + each][each1] = 1  # 没有的话，第一次加1\n",
    "        except:\n",
    "            names['dic_' + each] = dict.fromkeys(row, 1)  # 字段首次出现，创造字典\n",
    "\n",
    "\n",
    "    # 根据生成的计数字典生成矩阵\n",
    "    all_col = list(set(all_col0))   # 所有的字段（所有动物的集合）\n",
    "    all_col.sort(reverse=False)  # 给定词汇列表排序排序，为了和生成空矩阵的横向列名一致\n",
    "    df_final0 = pd.DataFrame(columns=all_col)  # 生成空矩阵\n",
    "    for each in all_col:  # 空矩阵中每列，存在给字段字典，转为一列存入矩阵，否则先创造全为零的字典，再填充进矩阵\n",
    "        try:\n",
    "            temp = pd.DataFrame(names['dic_' + each], index=[each])\n",
    "        except:\n",
    "            names['dic_' + each] = dict.fromkeys(all_col, 0)\n",
    "            temp = pd.DataFrame(names['dic_' + each], index=[each])\n",
    "        df_final0 = pd.concat([df_final0, temp])  # 拼接\n",
    "\n",
    "\n",
    "    df_final = df_final0.fillna(0)\n",
    "\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    temp1 = ['狗', '狮子', '孔雀', '猫咪']\n",
    "    temp2 = ['大象', '狮子', '老虎', '猪']\n",
    "    temp3 = ['大象', '北极熊', '老虎', '猪']\n",
    "    temp4 = ['大象', '狗', '老虎', '小鸡']\n",
    "    temp5 = ['孔雀', '狮子', '老虎', '猪']\n",
    "    temp_all = [temp2, temp1, temp3, temp4, temp5]\n",
    "    vol_li = pd.Series(temp_all)\n",
    "    df_matrix = gx_matrix(vol_li)\n",
    "    print(vol_li)\n",
    "    print(df_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='gbk'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'pages_content'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        for x in ['wrap','zw-con']:\n",
    "            news_list = soup.find('div', {'class': x})\n",
    "            if news_list!=None:\n",
    "                break\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)\n",
    "        \n",
    "        \n",
    "def web2(u):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'\n",
    "    }\n",
    "    url = u\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding='GB2312'\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    # 找出class属性值为pages_content的div\n",
    "    news_list = soup.find('div', {'class': 'zw-con'})\n",
    "    #pages_content wrap Custom_UnionStyle scy-main gsj_htmlcon warp  ccontent center\n",
    "    if news_list==None:\n",
    "        main_text_element = soup.find('td', {'class': 'black14'})\n",
    "        for style_element in soup.find_all('style'):\n",
    "            style_element.decompose()\n",
    "        \n",
    "        news_list=main_text_element\n",
    "    # 找出news_list下的所有p标签\n",
    "    news = news_list.find_all('p')\n",
    "\n",
    "    txt=open('try.txt','w')\n",
    "    for x in news:\n",
    "        a=x.text\n",
    "        txt.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def ff():\n",
    "        import requests\n",
    "        import json\n",
    "        from bs4 import BeautifulSoup\n",
    "        import re\n",
    "        import os\n",
    "        df = pd.read_excel(r'C:\\Users\\HONEYMOON\\Desktop\\规划\\中央创业政策2.xlsx')\n",
    "        web = df['网址']\n",
    "        web_lst = []\n",
    "        for w in web:\n",
    "            web_lst.append(w)\n",
    "        lst = []\n",
    "        dics_counts={}\n",
    "        for u in web_lst[:118]:#print(web_lst.index(u)+2)\n",
    "            r = requests.get(url=u, headers=headers)\n",
    "            r.encoding = r.apparent_encoding\n",
    "            text1 = r.text\n",
    "            soup = BeautifulSoup(text1, 'lxml')\n",
    "            txt = open('fff.txt', 'w', encoding='utf-8')\n",
    "            for each in soup.find_all('p'):\n",
    "                if each.string:\n",
    "                    fp.writelines(each.string)\n",
    "            txt.close()\n",
    "            txt = open('try.txt',encoding = 'utf-8').read()\n",
    "            #加载停用词表\n",
    "            stopwords = [line.strip() for line in open(r\"D:\\try\\停用词.txt\",encoding=\"utf-8\").readlines()]\n",
    "            words  = jieba.lcut(txt)\n",
    "            counts = {}\n",
    "            lst_top3=[]\n",
    "            for word in words:\n",
    "            #不在停用词表中\n",
    "                if word not in stopwords:\n",
    "                #不统计字数为一的词\n",
    "                    if len(word) == 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        counts[word] = counts.get(word,0) + 1\n",
    "            items = list(counts.items())\n",
    "            items.sort(key=lambda x:x[1], reverse=True)\n",
    "            for i in range(3):\n",
    "                word, count = items[i]\n",
    "                if word.isdigit():\n",
    "                    continue\n",
    "                else:\n",
    "                    dics_counts[word]=dics_counts.get(word,0)+1\n",
    "                    lst_top3.append(word)\n",
    "            lst.append(lst_top3)\n",
    "            dics_counts = dict(sorted(dics_counts.items(), key=lambda item:item[1], reverse=True))\n",
    "        print(dics_counts)\n",
    "        print(lst)\n",
    "        '''print(lst_top3)\n",
    "            print(counts)\n",
    "            print(items)\n",
    "            print()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-5cdd2d0a6a88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-ef48f466e41a>\u001b[0m in \u001b[0;36mff\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdics_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweb_lst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m118\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#print(web_lst.index(u)+2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapparent_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtext1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "ff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
